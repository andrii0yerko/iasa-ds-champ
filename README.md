# IASA DS Champ Test Task
Розв'язок задачі для відбіркового завдання хакатону IASA DS Champ від команди Team GARCH

# Summary
- [EDA](./notebooks/exploration.ipynb) - дослідження датасету, та змін розподілів з часом
- Model training - Multioutput LightGBM на кожен з таргетів, кросс-валідація по часу
- Model inferencing


# Report
## Аналіз задачі та даних
> Детальніше - [Блокнот з EDA](./notebooks/exploration.ipynb)

Маємо дані розподілені в часі (install_date), в цьому випадку доцільно валідувати моделі використовуючи перехресне розбиття за часом,
Навіть попри те, що в контексті цього завдання ми розглядаємо кожного користувача окремо, розподіли фіч та таргетів можуть змінюватись з часом через зовнішні обставини (Data Drift). Тому коректний вибір валідаційної схеми є важливим задля запобігання випадку коли ми валідуємо "прогнозуючи минуле по майбутньому", що є нонсенсом в контексті бізнес задачі, і фактично є оверфітом під вже зібраний датасет.

Наданий датасет покриває період грудень 2021 - січень 2022. Після дослідження бачимо, що розподіл кількості встановлень за день нерівномірний,
отже для кросс-валідації надамо перевагу розбиттю на рівномірні інтервали за датою встановлення.
Як довжину інтервалу було обрано значення в 7 днів.


## Метрика
- Оскільки ми прогнозуємо ltv, тобто грошову величину, в якості "основної" метрики доцільно обрати **MAE**, оскільки вона має ту ж одиницю виміру, що і оригінальна величина, та легка для розуміння, що дуже важливо коли мова йде про фінанси - це дає змогу одразу зрозуміти на яку суму модель пере-/недооцінила реальний прибуток, а також знаючи порядок точності моделі, закласти відповідні ризики.
- Додатковою метрикою слугує RMSE, оскільки внаслідок квадратичності, сильно зростає у випадку, якщо модель пере-/недооцінює ltv "китів", що теж є важливим.
- MAPE в цьому випадку є поганим вибором, оскільки набуває неінформативно великих значень через велику кількість користувачів з нульовим чи низьким ltv, а також є не дуже корисною з точки зору бізнес-логіки - помилка в 10 доларів, це вже ще помилка в 10 доларів, не залежно від того, скільки користувач приніс загалом.
- Також, в для більш загального розуміння якості прогнозу, ми вказуємо точність класифікації "нульових" користувачів, і окремо усі регресійні метрики на "ненульових"

## Модель
> Training - Coming soon \
> Inference - Coming soon

Фінальною цільовою змінною є сума трьох ltv різної природи, прогнозуватимемо кожне з них окремо, а потім сумуватимемо,
оскільки кожен таргет може залежати від різних ознак.

Як основну модель оберемо градієнтний бустинг з [LightGBM](https://lightgbm.readthedocs.io/en/v3.3.2/), оскільки ця категорія алгоритмів демонструє SOTA точність для більшості задач прогнозування на табличних даних, а також гарно узагальнює за рахунок ансамблюваня weak learner'ів (неглибоких дерев прийняття рішень) та внутрішніх механізмів регуляризації.
З особливостей - визначимо цільову функцію оптимізації як "[tweedie](https://en.wikipedia.org/wiki/Tweedie_distribution)", оскільки вона, на відміну від стандартної "mse", схильна демонструвати кращий результат у випадку,
коли розподіл цільової змінної має багато нулів та довгий хвіст.

### Feature Enginnering
Було створено певну кількість ознак, що повинні допомогти моделям на основі дерев знаходити залежності.

В їх числі:
- datetime фічі витягнуті з install_date, sin/cos представлення, день-місяць-рік, а також спеціальний флаг на свята та вихідні
- дельти. Для користувача дані значення різних показників за 1, 2, 3 ... день, але моделі важливі не абсолютні значення, а динаміку між ними, проте дерева через свою внутрішню структуру не можуть отримати такі різниці самостійно. Введемо різниці між послідовними значеннями як додаткові ознаки
- прогноз лінійної регресії. Для кожного з таргетів, побудуємо лінійну модель на значеннях цього ltv за 1, 2, 3 день (time-respecting, використовуючи те ж часове розбиття для тренування/прогнозування що й в основній моделі), та введемо значення її предикту, як ще одну ознаку. Це повинно допомогти моделі врахувати загальний тренд ltv користувачів.

### Результати
Coming soon

### Аналіз помилок
Coming soon
